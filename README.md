## ğŸ“ˆ Market Master â€” Componentâ€‘Weighted Index Signal (1â€‘Hour) MLOps Pipeline

A productionâ€‘quality local MLOps pipeline that predicts 5â€‘class trading actions per index constituent and aggregates them into a componentâ€‘weighted index signal (e.g., NASDAQâ€‘100 â†’ /NQ). Built with scikitâ€‘learn, MLflow (with Model Registry), FastAPI, and pytest.

---

## ğŸ“‹ Problem & Solution
Modern markets produce continuous OHLCV data, making multiâ€‘indicator decisions hard to execute consistently. We frame the problem at the index level using constituent signals:

### Operational framing (1â€‘hour constituents and index aggregation)
- Scope: Multiple symbols (index constituents, e.g., NASDAQâ€‘100/QQQ), intraday 1â€‘hour OHLCV candles.
- Horizon & cadence: Predict the next hourâ€™s action (t+1) per symbol after each completed hourly candle; aggregate to an index signal once per hour.
- History window: Use the latest W bars (e.g., 12â€“20 hours) for features; first W bars are warmâ€‘up (no signal).
- Labeling: Map nextâ€‘bar return to 5 classes via thresholds (t1 < t2). For 1â€‘hour bars, prefer dynamic thresholds (quantile- or volatility-based) validated via CV.
- Perâ€‘symbol outputs per hour: action, perâ€‘class probabilities, confidence (max prob), timestamp_next.
- Index aggregation: Convert actions to scores {strong_buy:+2, buy:+1, hold:0, sell:âˆ’1, strong_sell:âˆ’2}; compute Weighted Sentiment Score (WSS) = Î£(score_i Ã— weight_i).
- Index decision policy: If WSS â‰¥ +0.5 â†’ BUY futures (/NQ). If WSS â‰¤ âˆ’0.5 â†’ SELL futures. Otherwise HOLD. Tune thresholds by backtest.
- Batch mode: Provide 21â€‘day 1â€‘hour CSVs for all constituents â†’ receive perâ€‘symbol tables, WSS time series, and final hourly signals for offline evaluation/backtesting.
 - MVP scope: To move fast and cover the majority of index weight, we start with the topâ€‘10 QQQ constituents (normalized weights). This can be expanded to the full NASDAQâ€‘100 once the pipeline is validated.

### Solution overview (implemented)
- 1â€‘hour OHLCV for index constituents (topâ€‘10 QQQ in MVP).
- Pooled training across constituents with a symbol feature; multiple candidates compared; champion selected.
- Perâ€‘symbol predictions aggregated to WSS to emit an /NQ signal; logged to MLflow.
- Champion served via FastAPI, loading from MLflow Model Registry alias `@Production`.

### ğŸ’¼ Business impact
- **Consistency in decisions**: Clear actions (strong_sell/sell/hold/buy/strong_buy) per constituent reduce emotional noise.
- **Operational efficiency**: Automates multiâ€‘indicator synthesis; hourly indexâ€‘level signal for faster decisioning.
- **Risk awareness**: Perâ€‘class probabilities enable confidenceâ€‘based thresholds and risk controls.
- **Path to scale**: Structure extends to full NASDAQâ€‘100, registryâ€‘based promotion, and cloud later.

## ğŸš€ Features
- Pooled training and model comparison (RF/ET/GB/HGBT/MLP/SVC/LogReg) with MLflow nested runs; champion persisted.
- MLflow Model Registry integration with aliases (Staging/Production); API serves `models:/market-master-component-classifier@Production`.
- Perâ€‘symbol predictions and WSS aggregation for index signal; logs artifacts/metrics to MLflow.
- Monitoring with Evidently (drift and classification quality); gating criteria and promotion utility.
- FastAPI with `/health`, `GET /predict/component`, `GET /signal/index`, and `POST /predict`.

---

## ğŸ› ï¸ Stack

| Layer            | Tool(s)                                                                              |
|------------------|--------------------------------------------------------------------------------------|
| ğŸ§  Model         | scikitâ€‘learn candidates (RF/ET/GB/HGBT/MLP/SVC/LogReg), LabelEncoder                 |
| âš™ï¸ Features      | pandas, NumPy (returns, rolling stats, RSIâ€‘like, MA, ATR, BB width)                  |
| ğŸ“Š Monitoring    | Evidently (implemented)                                                               |
| âš™ï¸ Orchestration | Runner script (`src/run_pipeline.py`) + Windows Task Scheduler; Prefect 2 (optional) |
| ğŸ“¦ Tracking      | MLflow (SQLite) + Model Registry with aliases                                         |
| ğŸŒ API           | FastAPI + Uvicorn                                                                    |
| ğŸ›¢ï¸ Storage       | Local files (CSV, `artifacts/`, `mlruns/`)                                           |
| ğŸ§ª Testing       | pytest                                                                               |

---

## ğŸ³ Quickstart (Local)

1) Clone and enter the repo
```bash
git clone <your-repo-url>
cd market-master-trading-prediction-system
```

2) Create virtual environment
- Windows (PowerShell)
```powershell
py -3.10 -m venv .venv
.venv\Scripts\activate
python -m pip install -U pip
```
- macOS/Linux (bash)
```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
```

3) Install deps
```bash
pip install -r requirements.txt
```

4) Start MLflow UI (optional but recommended)
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --port 5000
# Open http://localhost:5000 in your browser
```

5) Run the endâ€‘toâ€‘end pipeline (fetch â†’ train/compare â†’ predict index â†’ monitor â†’ gate â†’ promote)
```bash
python -m src.run_pipeline --interval 1h --days 30 --max-symbols 10
```

6) Serve the API (loads Production model from MLflow registry with local fallback)
```bash
uvicorn src.api:app --host 0.0.0.0 --port 8000 --reload
# Open http://localhost:8000/docs for Swagger UI
```

7) Sample predictions
- Single component (latest history from CSV):
```bash
curl "http://localhost:8000/predict/component?symbol=NVDA"
```
- Index signal (WSS aggregation):
```bash
curl "http://localhost:8000/signal/index?universe=qqq"
```
- POST /predict with bars (JSON body):
```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
        "symbol": "NVDA",
        "interval": "1h",
        "bars": [
          {"open": 101.2, "high": 102.1, "low": 100.5, "close": 101.7, "volume": 123456}
        ]
      }'
```
Response includes `action` in {"strong_sell","sell","hold","buy","strong_buy"} and perâ€‘class probabilities.
```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"open": 101.2, "high": 102.1, "low": 100.5, "close": 101.7, "volume": 123456}'
```
Response will be a JSON with `action` in {"strong_sell","sell","hold","buy","strong_buy"} and per-class probabilities.

8) Run tests
```bash
pytest -q
```

---

## âš™ï¸ Project Structure
```bash
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ components/                  # perâ€‘symbol 1h OHLCV CSVs
â”‚   â””â”€â”€ weights/qqq_weights.csv      # normalized QQQ weights (topâ€‘10 in MVP)
â”œâ”€â”€ mlruns/                          # MLflow artifacts (created at run)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ features.py                  # indicators (returns, rolling stats, RSI, MA, ATR, BB width)
â”‚   â”œâ”€â”€ data.py                      # dataset utilities and metadata persistence
â”‚   â”œâ”€â”€ fetch_symbol.py              # robust yfinance fetcher with marketâ€‘hours filter
â”‚   â”œâ”€â”€ fetch_components.py          # orchestrates multiâ€‘symbol fetch
â”‚   â”œâ”€â”€ fetch_weights_qqq.py         # fetch/normalize QQQ weights (with fallback)
â”‚   â”œâ”€â”€ model_candidates.py          # candidate estimators for comparison
â”‚   â”œâ”€â”€ train_pooled_compare.py      # pooled training, MLflow logging, registry registration
â”‚   â”œâ”€â”€ predict_index.py             # perâ€‘symbol predictions â†’ WSS â†’ index signal
â”‚   â”œâ”€â”€ monitor_drift.py             # Evidently drift + classification quality reports
â”‚   â”œâ”€â”€ gate_and_report.py           # promotion gating and summary
â”‚   â”œâ”€â”€ registry.py                  # promote/rollback aliases (Staging/Production)
â”‚   â”œâ”€â”€ run_pipeline.py              # endâ€‘toâ€‘end runner (local orchestration)
â”‚   â”œâ”€â”€ hourly_predict.py            # hourly scheduler entrypoint (Windows Task Scheduler)
â”‚   â””â”€â”€ api.py                       # FastAPI serving (registryâ€‘first model load)
â””â”€â”€ tests/
    â”œâ”€â”€ test_features.py             # unit tests for feature funcs
    â””â”€â”€ test_api.py                  # API smoke test
```

---

## ğŸ“Š Models & Metrics

- **Target**: 5â€‘class action: {strong_sell, sell, hold, buy, strong_buy}.
- **Candidates**: RF/ET/GB/HGBT/MLP/SVC/LogReg trained on pooled dataset (oneâ€‘hot `symbol`).
- **Features**: logâ€‘returns, rolling mean/volatility, RSIâ€‘like momentum, MAs (short/long), ATR, Bollinger Band width, range ratios.
- **Selection**: macro F1 on timeâ€‘based split; accuracy as tiebreaker.
- **Logging (MLflow)**: params, metrics, perâ€‘candidate artifacts; champion registered as `market-master-component-classifier` with alias `Staging` â†’ gated promotion to `Production`.

---

## âœ… Evaluation Readiness
 The project is structured for easy grading and local execution.

- **Problem description**: Clearly defined 1â€‘hour constituent signals aggregated to an index decision.
- **Cloud**: Localâ€‘only. Docker/compose optional later.
- **Experiment tracking & registry**: MLflow tracking implemented; Model Registry with aliases implemented.
- **Workflow orchestration**: Local runner (`src/run_pipeline.py`) and Windows Task Scheduler for hourly runs; Prefect flow included as optional (versionâ€‘pin pending).
- **Model deployment**: FastAPI with `/health`, `/predict/component`, `/signal/index`, and `/predict`; loads registry `@Production` model with artifact fallback.
- **Monitoring**: Evidently drift and classification quality reports saved to `data/monitoring/` and logged to MLflow.
- **Reproducibility**: Pinned requirements, deterministic seeds, and tests.
- **Best practices**: Unit tests, API smoke test; gating and rollback utilities.

---

## ğŸ”§ Configuration (Local)
Environment variables are optional. Defaults are embedded for local use.
- `MLFLOW_TRACKING_URI`: `sqlite:///mlflow.db`
- `MODEL_URI`: `models:/market-master-component-classifier@Production` (API tries this first; falls back to `artifacts/model/model.pkl`)

---

## ğŸš† Execution Flow (Local)
1) Fetch: QQQ weights â†’ 1â€‘hour OHLCV per constituent to `data/components/`.
2) Features/labels: rolling indicators and dynamic thresholds; persist metadata.
3) Train/compare: candidates on pooled dataset; select champion; log to MLflow and register as Staging.
4) Predict index: perâ€‘symbol actions â†’ WSS â†’ /NQ signal; log table/WSS to MLflow.
5) Monitor: Evidently drift and classification quality reports; log to MLflow.
6) Gate: evaluate drift and F1 thresholds; if pass â†’ promote Staging â†’ Production.
7) Serve: API loads Production model via registry.

---

## ğŸ“Œ Acceptance Criteria
- Fresh clone â†’ set up â†’ run pipeline â†’ serve API â†’ call endpoints â†’ tests pass; no code edits required.
- MLflow UI shows runs for candidates, artifacts, WSS logs; Model Registry has aliases Staging/Production.
- API `/health` shows registry model; `/signal/index` returns WSS and signal.

---

## ğŸ—ºï¸ Roadmap
- Expand to full NASDAQâ€‘100 constituents and improve symbol metadata.
- Add probability calibration (e.g., Platt scaling) for selected models.
- Tighten gates (delta F1 vs champion, latency SLO) and autoâ€‘rollback on alerts.
- Prefect pinning and UI deployment once dependency versions are aligned.
- Docker Compose for API + MLflow UI; minimal CI to run tests on push.


