## ğŸ“ˆ Market Master â€” Trading Action Prediction MLOps Pipeline

A production-quality MLOps pipeline for multi-class trading action prediction â€” built with scikit-learn, MLflow, FastAPI, and pytest.

This project demonstrates the full local ML lifecycle: from training and experiment tracking to serving a prediction API and running tests. We now plan to scale from a single-symbol 5â€‘minute prototype to a real fintech scenario using 1â€‘hour OHLCV and a componentâ€‘weighted index signal (e.g., NASDAQâ€‘100 â†’ /NQ).

---

## ğŸ“‹ Problem & Solution
Modern markets produce continuous OHLCV data, making multi-indicator decisions hard to execute consistently.
- Phase A (done): single-symbol 5â€‘class action prediction from recent OHLCV.
- Phase B (next): componentâ€‘weighted index signal using 1â€‘hour OHLCV for index constituents (e.g., NASDAQâ€‘100) and a pooled champion classifier.

Our pipeline trains classifiers, logs to MLflow, serves a champion via FastAPI, and will aggregate perâ€‘component predictions into an index futures signal.

### Operational framing (Phase A, example: NVDA, 5â€‘minute bars)
- **Scope**: Single asset, intraday 5â€‘minute OHLCV candles.
- **Horizon & cadence**: Predict the next barâ€™s action (t+1) after each completed candle; ~78 predictions per full trading day.
- **History window**: Use the latest W bars (e.g., 20â€“60) to compute rolling features; first W bars are warmâ€‘up (no signal).
- **Labeling**: Forward return r_next mapped to 5 classes via thresholds t1 < t2 (strong_sell, sell, hold, buy, strong_buy) with no lookâ€‘ahead.
- **API outputs per bar**: `action`, perâ€‘class `probabilities`, `confidence` (max prob), `timestamp_next`, and `model_version` metadata.
- **Example decision policy**: If `confidence â‰¥ 0.6`, act per class mapping; otherwise `hold`. Pair with risk controls (position limits, SL/TP, flat by close).
- **Batch mode**: Provide a 10â€‘day CSV â†’ receive a table of timestamps, actions, and probabilities for offline evaluation/backtesting.

### Operational framing (Phase B, 1â€‘hour constituents and index aggregation)
- Scope: Multiple symbols (index constituents, e.g., NASDAQâ€‘100/QQQ), intraday 1â€‘hour OHLCV candles.
- Horizon & cadence: Predict the next hourâ€™s action (t+1) per symbol after each completed hourly candle; aggregate to an index signal once per hour.
- History window: Use the latest W bars (e.g., 12â€“20 hours) for features; first W bars are warmâ€‘up (no signal).
- Labeling: Map nextâ€‘bar return to 5 classes via thresholds (t1 < t2). For 1â€‘hour bars, prefer dynamic thresholds (quantile- or volatility-based) validated via CV.
- Perâ€‘symbol outputs per hour: action, perâ€‘class probabilities, confidence (max prob), timestamp_next.
- Index aggregation: Convert actions to scores {strong_buy:+2, buy:+1, hold:0, sell:âˆ’1, strong_sell:âˆ’2}; compute Weighted Sentiment Score (WSS) = Î£(score_i Ã— weight_i).
- Index decision policy: If WSS â‰¥ +0.5 â†’ BUY futures (/NQ). If WSS â‰¤ âˆ’0.5 â†’ SELL futures. Otherwise HOLD. Tune thresholds by backtest.
- Batch mode: Provide 21â€‘day 1â€‘hour CSVs for all constituents â†’ receive perâ€‘symbol tables, WSS time series, and final hourly signals for offline evaluation/backtesting.

### Solution overview
- Phase A (done): focused features; 5â€‘class labels; RF baseline with candidate comparison (RF/ET/GB/HGBT/MLP/SVC/LogReg); MLflow + FastAPI; tests.
- Phase B (next): 1â€‘hour OHLCV for index constituents; perâ€‘symbol predictions + weighted aggregation (WSS) to emit /NQ signal; pooled training with symbol feature; championâ€“challenger.

### Phase B (in progress)
- Switching to 1â€‘hour OHLCV and pooled training across index constituents (e.g., NASDAQâ€‘100).
- Aggregating perâ€‘symbol actions to WSS and emitting /NQ signals; logging perâ€‘symbol tables and WSS to MLflow.

### ğŸ’¼ Business impact (single-asset local prototype)
- **Consistency in decisions**: Map indicator signals to clear actions (strong_sell/sell/hold/buy/strong_buy), reducing emotional noise.
- **Operational efficiency**: Automates multi-indicator synthesis, reducing manual analysis time.
- **Risk awareness**: Multi-class probabilities support confidence-based thresholds and risk controls.
- **Path to scale**: Validated locally, the same structure extends to model registry, monitoring, and cloud later.

## ğŸš€ Features
- **Training script** that: loads CSV, engineers practical indicators, trains a scikit-learn classifier for 5 actions, logs metrics and artifacts to local MLflow.
- **Prediction API (FastAPI)** that loads the saved model artifact (or via MLflow URI) and returns one of {strong_sell, sell, hold, buy, strong_buy} with class probabilities.
- **Tests (pytest)** for features and API smoke tests.
- **Reproducible runs** with a tiny sample dataset in `data/raw/` and local MLflow in `./mlruns`.

---

## ğŸ› ï¸ Stack

| Layer            | Tool(s)                                                                              |
|------------------|--------------------------------------------------------------------------------------|
| ğŸ§  Model         | scikit-learn (RandomForest baseline), LabelEncoder for action labels                 |
| âš™ï¸ Features      | pandas, NumPy (OHLCV-derived: returns, rolling stats, RSI-like, MA, ATR, BB width)  |
| ğŸ“Š Monitoring    | Evidently (planned)                                                                  |
| âš™ï¸ Orchestration | Prefect 2 (planned)                                                                  |
| ğŸ“¦ Tracking      | MLflow (local `./mlruns`, SQLite backend by default)                                 |
| ğŸŒ API           | FastAPI + Uvicorn                                                                    |
| ğŸ›¢ï¸ Storage       | Local files (CSV, `artifacts/`); PostgreSQL via Docker Compose (optional, later)     |
| ğŸ§ª Testing       | pytest                                                                               |

Items marked â€œplannedâ€ will be added after the core local loop is stable and verified.

---

## ğŸ³ Quickstart (Local)

1) Clone and enter the repo
```bash
git clone <your-repo-url>
cd market-master-trading-prediction-system
```

2) Create virtual environment
- Windows (PowerShell)
```powershell
py -3.10 -m venv .venv
.venv\Scripts\activate
python -m pip install -U pip
```
- macOS/Linux (bash)
```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
```

3) Install deps (we will add `requirements.txt` next)
```bash
pip install -r requirements.txt
```

4) (Optional) Start MLflow UI
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --port 5000
# Open http://localhost:5000 in your browser
```

5) Train the model (script coming in `src/train.py`)
```bash
python src/train.py
```
This will: read `data/raw/sample_ohlcv.csv`, create features, train a 5-class model, and log params/metrics/artifacts to MLflow. The final model is also saved to `artifacts/model/`.

6) Serve the API
```bash
uvicorn src.api:app --host 0.0.0.0 --port 8000 --reload
# Open http://localhost:8000/docs for Swagger UI
```

7) Sample prediction
```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"open": 101.2, "high": 102.1, "low": 100.5, "close": 101.7, "volume": 123456}'
```
Response will be a JSON with `action` in {"strong_sell","sell","hold","buy","strong_buy"} and per-class probabilities.

8) Run tests
```bash
pytest -q
```

---

## âš™ï¸ Project Structure
```bash
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt                 # to be added
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ sample_ohlcv.csv         # tiny sample dataset (to be added)
â”œâ”€â”€ mlruns/                          # local MLflow artifacts (created at run)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py                      # load/split data
â”‚   â”œâ”€â”€ features.py                  # indicators (returns, rolling stats, RSI, MA, ATR, BB width)
â”‚   â”œâ”€â”€ train.py                     # train multi-class model + log to MLflow + persist artifact
â”‚   â”œâ”€â”€ infer.py                     # CLI/batch inference
â”‚   â””â”€â”€ api.py                       # FastAPI serving (loads latest model)
â””â”€â”€ tests/
    â”œâ”€â”€ test_features.py             # unit tests for feature funcs
    â””â”€â”€ test_api.py                  # API smoke test
```

---

## ğŸ“Š Models & Metrics

- **Target**: 5-class action label: {strong_sell, sell, hold, buy, strong_buy}.
- **Baseline model**: `RandomForestClassifier` (robust default for tabular features).
- **Alternative**: `LogisticRegression` (simple/explainable baseline for comparison).
- **Features**: log-returns, rolling mean/volatility, momentum (RSI-like), moving averages (short/long), ATR, Bollinger Band width, and simple price range ratios.
- **Metrics logged (MLflow)**: overall accuracy, macro F1, per-class precision/recall/F1, confusion matrix artifact, and PR/ROC where applicable.

Example outcomes from earlier prototypes are directional targets only. Actual results depend on dataset, interval (5â€‘min vs 1â€‘hour), and seeds.

---

## âœ… Evaluation Readiness (per `project-requirement.txt`)
 The project follows the rubric in `project-requirement.txt` and is built to be graded easily.

- **Problem description**: Clearly defined, narrow trading objective (5-class trading actions for one asset) with rationale and constraints.
- **Cloud**: Local-only for now (0â€“2 points). A minimal, optional Dockerization and later registry can enable easy promotion to cloud when weâ€™re ready.
- **Experiment tracking and model registry**: MLflow tracking enabled from day one. Registry to be added after confirmation (current: tracking; target: tracking + registry).
- **Workflow orchestration**: Initially manual CLI steps for clarity. A simple workflow (e.g., Prefect) can be added to run train â†’ evaluate â†’ persist as one flow after local path is verified.
- **Model deployment**: Local FastAPI service with `/predict` and `/health` and a repeatable model load path.
- **Model monitoring**: Basic local metrics via MLflow; we plan to add a minimal Evidently drift report generated on a held-out slice.
- **Reproducibility**: Clear instructions, pinned `requirements.txt`, tiny sample dataset included, deterministic seeds, and tests.
- **Best practices**: Unit tests, integration (API) test, formatter/linter, optional Makefile, and later a minimal CI on push.

We will mark each criterion as â€œImplementedâ€ or â€œPlannedâ€ in the repository once we add the corresponding files.

---

## ğŸ”§ Configuration (Local)
Environment variables are optional. Reasonable defaults will be embedded for local use.
- `MLFLOW_TRACKING_URI`: default `file:./mlruns`
- `MODEL_URI`: default to latest saved artifact in `artifacts/model/`

---

## ğŸš† Execution Flow (Local)
1) Data: read `data/raw/sample_ohlcv.csv` â†’ drop NAs â†’ train/test split.
2) Features: compute small set of indicators (no over-engineering).
3) Train: scikit-learn baseline (e.g., LogisticRegression or RandomForest) â†’ log params/metrics/artifacts to MLflow.
4) Serve: load saved model (local path or `models:/name@alias` when we enable registry) â†’ expose `/health` and `/predict`.
5) Test: unit tests for features, an API smoke test, and a tiny integration check in CI later.

---

## ğŸ“Œ Acceptance Criteria
- Fresh clone â†’ set up â†’ train â†’ serve API â†’ make a prediction â†’ tests pass, all on a local machine, without editing code.
- MLflow UI shows at least one run with 5-class metrics and a stored model artifact.
- No unused services, no broken instructions.

---

## ğŸ—ºï¸ Roadmap (post-confirmation)
- Add a tiny `Makefile`/`tasks.py` for convenience (optional on Windows).
- Add MLflow Model Registry and `@Production` alias for serving.
- Add basic monitoring (Evidently) and a drift report generated locally.
- Add Prefect flow for fetch â†’ train_compare â†’ gate â†’ promote â†’ predict_index.
- Add Docker for local reproducibility (compose for MLflow UI + API).
- Only after all the above are stable, consider a minimal cloud target.


